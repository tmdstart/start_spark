{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8075e070-fc63-4232-9a8b-9412c0bce5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print('hello')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a039c3b8-7efc-4ad7-847a-6f287dabd981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/conda/bin/python\n"
     ]
    }
   ],
   "source": [
    "!which python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8812db67-7881-4eeb-8663-d09096d50517",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.11.6\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6df7d63f-c757-4fee-bbeb-2c52ae7057b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "openjdk version \"17.0.8.1\" 2023-08-24\n",
      "OpenJDK Runtime Environment (build 17.0.8.1+1-Ubuntu-0ubuntu122.04)\n",
      "OpenJDK 64-Bit Server VM (build 17.0.8.1+1-Ubuntu-0ubuntu122.04, mixed mode, sharing)\n"
     ]
    }
   ],
   "source": [
    "!java -version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "914eb46f-bcb9-42af-94a9-23ae90c5b537",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/bin/java\n"
     ]
    }
   ],
   "source": [
    "!which java"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "264b380d-3f83-4313-bd2f-59a4201c59e9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: pyspark\n",
      "Version: 3.5.0\n",
      "Summary: Apache Spark Python API\n",
      "Home-page: https://github.com/apache/spark/tree/master/python\n",
      "Author: Spark Developers\n",
      "Author-email: dev@spark.apache.org\n",
      "License: http://www.apache.org/licenses/LICENSE-2.0\n",
      "Location: /usr/local/spark/python\n",
      "Requires: py4j\n",
      "Required-by: \n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip show pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "635ca389-1052-4cb3-807e-fd970a5c2982",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/spark'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#SPARK_HONE\n",
    "import os\n",
    "os.environ.get('SPARK_HOME')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "12059b13-a609-4568-9384-71e92d04cc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n"
     ]
    }
   ],
   "source": [
    "java_home = os.environ.get('JAVA_HOME')\n",
    "print(java_home)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "12a1686c-d487-4be2-9538-df6003f5a8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip:/usr/local/spark/python:'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.environ.get('PYTHONPATH')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef4b6233-9888-49eb-9c84-80feed4c09a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6eb9444e-13bb-4c96-9e0d-6cbfb8987c2b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark=SparkSession.builder.appName('pyspark example1').getOrCreate() #chaining\n",
    "#Sparkcontext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "483ef8be-132a-44d6-8f56-f00bb1af258a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://916f9437356c:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark example1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ffb1737e950>"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "643573a6-36c1-4d74-ba13-e9abf75f1cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop() #주기적으로 잡아줘야한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "27b3fe8c-d50c-456f-a8b0-9304be3e997a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('pyspark example1').getOrCreate() #chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "711cd9d7-9a60-48d1-83e0-dd727badae75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://916f9437356c:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark example1</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7ffb033c5e50>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "97a48e62-ec6f-46d0-ab30-7b264d546d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [('alice',1), ('Bob',2), ('Charlie',3), ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "53d9e097-1e6e-4157-a221-554da58753ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "fec09df7-130a-49a8-ae3d-6d474e57d043",
   "metadata": {},
   "outputs": [],
   "source": [
    "#DataFrame 객체(분산객체)를 생성 <> 판다스의 데이터프레임이 아님\n",
    "data1=spark.createDataFrame(data, ['Name','Value'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "21f6bef9-12c1-4eee-8265-f91b55dc6a73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Value: bigint]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1 #spark의 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "c301b8d5-3295-4e53-9494-6680ce3b4b9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'Value[1]'>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1[1][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03f38ca2-f585-4682-a7cc-e9c6bdf7f49e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#RDD spark이가지고 있는데이터 타입"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "5e40745c-e64a-4f2a-805a-92a947d084c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Value|\n",
      "+-------+-----+\n",
      "|  alice|    1|\n",
      "|    Bob|    2|\n",
      "|Charlie|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ab3265c-32c2-4cb3-9fe0-0cf0511cfde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#데이터 생성 RDD 객체 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "301f8e7c-a426-4950-beda-b2d268973d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark=SparkSession.builder.appName('pyspark example1').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "0b157088-8db0-4abe-958b-f8cdfa137460",
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = spark.sparkContext.parallelize([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "ba5a6f27-6f1d-4aa9-9404-3b672f3c3367",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[11] at readRDDFromFile at PythonRDD.scala:289"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd # 병렬로 관리되는 RDD 객체"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "80640ee9-d565-4a19-9897-cf8b2558a067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 4, 5]"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(6) #show와 같다 rdd객체를출력하는 함수 = n개를 지정 병렬로 분산되어 처리하고 있다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "483f2b6c-636f-4ba8-9786-2a000d3e0a47",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[26] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map연산 : rdd 값으로 연산\n",
    "squarded_rdd = rdd.map(lambda x:x*x)\n",
    "squarded_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "d6d85440-4a71-4aef-a6ef-5b70441d1064",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PythonRDD[26] at RDD at PythonRDD.scala:53"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squarded_rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "9713092f-5374-45c8-bea0-aaa3a4459353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squarded_rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "057f2096-e5f7-469f-b7ec-b59c3e2686e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "97c7c53f-a5cd-433e-80ea-f26172ee1560",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 4, 9, 16, 25]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "squarded_rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "a130b3af-0157-417f-8d16-0c7e68f5e944",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Name|Value|\n",
      "+----+-----+\n",
      "| Bob|    2|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.filter(data1.Name=='Bob').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "d78717fb-e5aa-475d-9902-3c945946c9fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Value: bigint]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.filter(data1.Value >2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1437ae89-9a4d-49aa-977a-3637ef6a8689",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Value|\n",
      "+-------+-----+\n",
      "|Charlie|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data1.filter(data1.Value >2).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dae99847-a12f-41af-956e-c5acb3d89ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data1.createOrReplaceTempView('people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "f1961d50-eed6-4937-8b58-71fc3e4be973",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Value: bigint]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select * from people')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "147db962-0917-4a32-9d60-c0dfdb4ba846",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Value|\n",
      "+-------+-----+\n",
      "|  alice|    1|\n",
      "|    Bob|    2|\n",
      "|Charlie|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from people').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "19af2500-faea-4c5e-99c7-0559e938c7a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Name|Value|\n",
      "+----+-----+\n",
      "| Bob|    2|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from people where value=2').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "3aa886c5-d16e-4aba-8e65-75a9e9271f2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+\n",
      "|Name|Value|\n",
      "+----+-----+\n",
      "| Bob|    2|\n",
      "+----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from people where Name=\"Bob\"').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "85120511-888c-4aac-8f00-53ea8cf93cbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----+\n",
      "|   Name|Value|\n",
      "+-------+-----+\n",
      "|Charlie|    3|\n",
      "+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql('select * from people where value=3').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a15e19-70f6-4bd2-8289-ae8616c02e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#MLLib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "95b6e002-6436-4b6a-b592-b363707e1523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "7e63da17-174d-40d3-8dd1-8e5208f5754e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "9cbbfee1-02ea-4cc7-a3fa-087fd1a06f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "76f2e5b4-7664-4b3e-97ba-c9bdca8f187a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_age= [('Alice', 25), ('Bob',30), ('CHarlie',33)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "7ecac4d9-e4f2-4d15-91a4-87f940f00a34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: bigint]"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2=spark.createDataFrame(data_age,['Name', 'Age'])\n",
    "data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "68a28ffa-a74d-4cd5-a55d-4ff5794e5e18",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler =VectorAssembler(inputCols = ['Age'], outputCol='features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "aaa6371c-dfb6-4359-819a-2bc1bbfff8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_df=assembler.transform(data2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "966f9b0a-f2b0-494a-abec-7aaff5645f72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: bigint, features: vector]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "c6706f3f-7646-4b18-8dd3-331073f7b8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr=LinearRegression(featuresCol ='features', labelCol='Age')\n",
    "model =lr.fit(vector_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c7cacec4-a2c7-4571-959c-d6235c5ddabf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Name: string, Age: bigint, features: vector, prediction: double]"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred=model.transform(vector_df)\n",
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "1739013e-5d55-4e9c-9886-b74a44dfb5b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---+--------+-----------------+\n",
      "|   Name|Age|features|       prediction|\n",
      "+-------+---+--------+-----------------+\n",
      "|  Alice| 25|  [25.0]|24.99999999999993|\n",
      "|    Bob| 30|  [30.0]|30.00000000000001|\n",
      "|CHarlie| 33|  [33.0]|33.00000000000006|\n",
      "+-------+---+--------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pred.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "de2acce2-dc73-44c1-b7ed-f0801bf62965",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "1c8e772e-d508-4926-a9ca-9140580709bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Streaming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27a63f2d-908f-447a-9814-5f00e5445c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import explode\n",
    "from pyspark.sql.functions import split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "77c760bf-8ae4-4a79-a6d6-a45236ff7749",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m spark \u001b[38;5;241m=\u001b[39m \u001b[43mSparkSession\u001b[49m\u001b[38;5;241m.\u001b[39mbuilder\u001b[38;5;241m.\u001b[39mappName(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyspark example1\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mgetOrCreate() \u001b[38;5;66;03m#chaining\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName('pyspark example1').getOrCreate() #chaining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "143b5b32-417f-4a8a-a5e5-893d1b84a255",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o928.load.\n: java.lang.IllegalStateException: No active or default Spark session found\n\tat org.apache.spark.sql.SparkSession$.$anonfun$active$2(SparkSession.scala:1202)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession$.$anonfun$active$1(SparkSession.scala:1202)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession$.active(SparkSession.scala:1201)\n\tat org.apache.spark.sql.execution.streaming.sources.TextSocketSourceProvider.getTable(TextSocketSourceProvider.scala:63)\n\tat org.apache.spark.sql.internal.connector.SimpleTableProvider.getOrLoadTable(SimpleTableProvider.scala:35)\n\tat org.apache.spark.sql.internal.connector.SimpleTableProvider.inferSchema(SimpleTableProvider.scala:41)\n\tat org.apache.spark.sql.internal.connector.SimpleTableProvider.inferSchema$(SimpleTableProvider.scala:39)\n\tat org.apache.spark.sql.execution.streaming.sources.TextSocketSourceProvider.inferSchema(TextSocketSourceProvider.scala:38)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:90)\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:180)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:145)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[161], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lines \u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadStream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msocket\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mhost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocalhost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moption\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mport\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m9999\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#streamiming 데이터가 들어온다.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/sql/streaming/readwriter.py:304\u001b[0m, in \u001b[0;36mDataStreamReader.load\u001b[0;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[1;32m    302\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jreader\u001b[38;5;241m.\u001b[39mload(path))\n\u001b[1;32m    303\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 304\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m/usr/local/spark/python/pyspark/errors/exceptions/captured.py:179\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdeco\u001b[39m(\u001b[38;5;241m*\u001b[39ma: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[1;32m    178\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 179\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkw\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m Py4JJavaError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    181\u001b[0m         converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n",
      "File \u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9.7-src.zip/py4j/protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[1;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[1;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[1;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[1;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o928.load.\n: java.lang.IllegalStateException: No active or default Spark session found\n\tat org.apache.spark.sql.SparkSession$.$anonfun$active$2(SparkSession.scala:1202)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession$.$anonfun$active$1(SparkSession.scala:1202)\n\tat scala.Option.getOrElse(Option.scala:189)\n\tat org.apache.spark.sql.SparkSession$.active(SparkSession.scala:1201)\n\tat org.apache.spark.sql.execution.streaming.sources.TextSocketSourceProvider.getTable(TextSocketSourceProvider.scala:63)\n\tat org.apache.spark.sql.internal.connector.SimpleTableProvider.getOrLoadTable(SimpleTableProvider.scala:35)\n\tat org.apache.spark.sql.internal.connector.SimpleTableProvider.inferSchema(SimpleTableProvider.scala:41)\n\tat org.apache.spark.sql.internal.connector.SimpleTableProvider.inferSchema$(SimpleTableProvider.scala:39)\n\tat org.apache.spark.sql.execution.streaming.sources.TextSocketSourceProvider.inferSchema(TextSocketSourceProvider.scala:38)\n\tat org.apache.spark.sql.execution.datasources.v2.DataSourceV2Utils$.getTableFromProvider(DataSourceV2Utils.scala:90)\n\tat org.apache.spark.sql.streaming.DataStreamReader.loadInternal(DataStreamReader.scala:180)\n\tat org.apache.spark.sql.streaming.DataStreamReader.load(DataStreamReader.scala:145)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:77)\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:568)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:374)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\n\tat java.base/java.lang.Thread.run(Thread.java:833)\n"
     ]
    }
   ],
   "source": [
    "lines = spark.readStream.format('socket').option('host', 'localhost').option('port', 9999).load() #streamiming 데이터가 들어온다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "15b00725-7da5-4df5-8e8e-3fd246178716",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "incomplete input (3548340976.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[163], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    words=lines.select(explode(split(lines.value, '')).alias('word')\u001b[0m\n\u001b[0m                                                                    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m incomplete input\n"
     ]
    }
   ],
   "source": [
    "words=lines.select(explode(split(lines.value, '')).alias('word')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "ae6c2510-a30e-4244-9a2a-7f06b648c5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
